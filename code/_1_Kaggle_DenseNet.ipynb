{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a832202e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b10cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import torch\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "import keras.backend as K\n",
    "from keras import layers\n",
    "from keras import backend\n",
    "from keras import models\n",
    "from keras import utils as keras_utils\n",
    "from keras_applications import imagenet_utils\n",
    "from keras_applications.imagenet_utils import decode_predictions\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.layers import Dense,Dropout,Conv2D,Input,MaxPool2D,Flatten,Activation, GlobalAveragePooling2D, BatchNormalization, MaxPooling2D, Conv2D, Concatenate\n",
    "from keras.models import Model\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c7862",
   "metadata": {},
   "source": [
    "# <center>EXPLORE Kaggle DATASET</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf97069",
   "metadata": {},
   "source": [
    "# Kaggle Dataset\n",
    "https://www.kaggle.com/datasets/niharika41298/yoga-poses-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8dcdce",
   "metadata": {},
   "source": [
    "## > Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21809682",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/kaggle_yogaposes/DATASET/TRAIN/'  \n",
    "\n",
    "# data = []\n",
    "# folder = []\n",
    "# thisdir = os.getcwd()\n",
    "\n",
    "# for r, d, f in os.walk(path):    \n",
    "#     for dic in d:\n",
    "#         path1 = path + dic + '/'\n",
    "#         for r2, d2, f2 in os.walk(path1):\n",
    "#             for file in f2:\n",
    "#                 data.append((dic, file))\n",
    "            \n",
    "# train_df = pd.DataFrame(data,columns=['YogaPoses','ImageNumbers'])\n",
    "# train_df['label of class_6']= train_df['YogaPoses'].astype('category').cat.codes\n",
    "# train_df['label of class_20'] = train_df['YogaPoses'].astype('category').cat.codes\n",
    "# train_df['label of class_82'] = train_df['YogaPoses'].astype('category').cat.codes\n",
    "# train_df.to_csv('../data/kaggle_yogaposes/DATASET/ALL/' + 'Train_df.csv')\n",
    "train_df = pd.read_csv('../data/kaggle_yogaposes/DATASET/ALL/' + 'Train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b4ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in provided training data:  5\n"
     ]
    }
   ],
   "source": [
    "print('classes in provided training data: ',len(train_df['YogaPoses'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96119186",
   "metadata": {},
   "source": [
    "## > Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e26ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/kaggle_yogaposes/DATASET/TEST/'  \n",
    "\n",
    "# data = []\n",
    "# folder = []\n",
    "# thisdir = os.getcwd()\n",
    "\n",
    "# for r, d, f in os.walk(path):    \n",
    "#     for dic in d:\n",
    "#         path1 = path + dic + '/'\n",
    "#         for r2, d2, f2 in os.walk(path1):\n",
    "#             for file in f2:\n",
    "#                 data.append((dic, file))\n",
    "            \n",
    "# train_df = pd.DataFrame(data,columns=['YogaPoses','ImageNumbers'])\n",
    "# train_df['label of class_6']= train_df['YogaPoses'].astype('category').cat.codes\n",
    "# train_df['label of class_20'] = train_df['YogaPoses'].astype('category').cat.codes\n",
    "# train_df['label of class_82'] = train_df['YogaPoses'].astype('category').cat.codes\n",
    "#test_df.to_csv('../data/kaggle_yogaposes/DATASET/ALL/' + 'Test_df.csv')\n",
    "test_df = pd.read_csv('../data/kaggle_yogaposes/DATASET/ALL/' + 'Test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70a44f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in provided test data:  5\n"
     ]
    }
   ],
   "source": [
    "print('classes in provided test data: ', len(test_df['YogaPoses'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0137fb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3032839665164198, 0.6967160334835801)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape[0] / (train_df.shape[0] + test_df.shape[0]), train_df.shape[0] / (train_df.shape[0] + test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7ac725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(471, 1082)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape[0], train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bc3d2",
   "metadata": {},
   "source": [
    "# Class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8f6e2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TestGrouped_df = test_df[['ImageNumbers', 'YogaPoses']].groupby(['YogaPoses']).count()\n",
    "TrainGrouped_df = train_df[['ImageNumbers', 'YogaPoses']].groupby(['YogaPoses']).count()\n",
    "df = pd.merge(TestGrouped_df, TrainGrouped_df, on='YogaPoses')\n",
    "df = df.rename(columns={\"ImageNumbers_x\": \"Test set\", \"ImageNumbers_y\": \"Training set\"})\n",
    "df['YogaPoses']=df.index\n",
    "df.reset_index\n",
    "\n",
    "df.plot.bar(x='YogaPoses', rot=90, figsize=(15,5), color=[\"mediumaquamarine\", \"lightgray\"])\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.title('Class Balance')\n",
    "plt.savefig('../Plots/Kaggle/ClassBalance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d83bce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3076923076923077, 0.30131004366812225)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TestRatio'] = df['Test set'] / (df['Test set'] + df['Training set'])\n",
    "df['TestRatio'].max(), df['TestRatio'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833c0bf",
   "metadata": {},
   "source": [
    "# <center>Modified Densenet-201 architecture</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21fd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CREATE TRIAL DATA TO BUILD THE MODEL'''\n",
    "\n",
    "'''The model was run on the ITU HPC cluster with the entire data set.'''\n",
    "\n",
    "TryTrain_df = train_df.groupby(['YogaPoses']).sample(8)\n",
    "TryTest_df = test_df.groupby(['YogaPoses']).sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1856b4",
   "metadata": {},
   "source": [
    "# keras_densenet_modified.py\n",
    "\n",
    "This code is based on the git hub repository of: \n",
    "\n",
    "Verma, M., Kumawat, S., Nakashima, Y., & Raman, S. (2020). Yoga-82: a new dataset for fine-grained classification of human poses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (pp. 1038-1039).\n",
    "\n",
    "https://github.com/maniver7/yoga-82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec3cc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_WEIGTHS_PATH = (\n",
    "    'https://github.com/keras-team/keras-applications/')\n",
    "\n",
    "DENSENET201_WEIGHT_PATH_NO_TOP = (\n",
    "    BASE_WEIGTHS_PATH +\n",
    "    'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "def dense_block(x, blocks, name):\n",
    "    \"\"\"A dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, 32, name=name + '_block' + str(i + 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, reduction, name):\n",
    "    \"\"\"A transition block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                  name=name + '_bn')(x)\n",
    "    x = layers.Activation('relu', name=name + '_relu')(x)\n",
    "    x = layers.Conv2D(int(backend.int_shape(x)[bn_axis] * reduction), 1,\n",
    "                      use_bias=False,\n",
    "                      name=name + '_conv')(x)\n",
    "    x = layers.AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    \"\"\"A building block for a dense block.\n",
    "\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis,\n",
    "                                   epsilon=1.001e-5,\n",
    "                                   name=name + '_0_bn')(x)\n",
    "    x1 = layers.Activation('relu', name=name + '_0_relu')(x1)\n",
    "    x1 = layers.Conv2D(4 * growth_rate, 1,\n",
    "                       use_bias=False,\n",
    "                       name=name + '_1_conv')(x1)\n",
    "    x1 = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                   name=name + '_1_bn')(x1)\n",
    "    x1 = layers.Activation('relu', name=name + '_1_relu')(x1)\n",
    "    x1 = layers.Conv2D(growth_rate, 3,\n",
    "                       padding='same',\n",
    "                       use_bias=False,\n",
    "                       name=name + '_2_conv')(x1)\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def DenseNet(blocks,\n",
    "             include_top=True,\n",
    "             weights='imagenet',\n",
    "             input_tensor=None,\n",
    "             input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=1000,\n",
    "             **kwargs):\n",
    "    \"\"\"Instantiates the DenseNet architecture.\n",
    "\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: numbers of building blocks for the four dense layers.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `'channels_last'` data format)\n",
    "            or `(3, 224, 224)` (with `'channels_first'` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional block.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional block, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    #global backend, layers, models, keras_utils\n",
    "    #backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name='conv1/conv')(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n",
    "    x = layers.Activation('relu', name='conv1/relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name='pool1')(x)\n",
    "\n",
    "    x = dense_block(x, blocks[0], name='conv2')\n",
    "    x = transition_block(x, 0.5, name='pool2')\n",
    "    x = dense_block(x, blocks[1], name='conv3')\n",
    "    \n",
    "    x = transition_block(x, 0.5, name='pool3')\n",
    "    x1=x\n",
    "    x = dense_block(x, blocks[2], name='conv4')\n",
    "    \n",
    "    x = transition_block(x, 0.5, name='pool4')\n",
    "    x2=x\n",
    "    x = dense_block(x, blocks[3], name='conv5')\n",
    "\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n",
    "    x = layers.Activation('relu', name='relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = tf.keras.utils.get_source_inputs(input_tensor) #tf.keras.utils.get_source_inputs\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, [x1,x2,x], name='densenet201_hir')\n",
    "    \n",
    "    # Load weights.\n",
    "    \n",
    "    if weights == 'imagenet':\n",
    "        \n",
    "        weights_path = keras_utils.get_file(\n",
    "            'densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "            DENSENET201_WEIGHT_PATH_NO_TOP,\n",
    "            cache_subdir='models',\n",
    "            file_hash='c13680b51ded0fb44dff2d8f86ac8bb1')\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "    #elif weights is not None:\n",
    "    #    model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def DenseNet201_hir(include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000,\n",
    "                **kwargs):\n",
    "    return DenseNet([6, 12, 48, 32],\n",
    "                    include_top, weights,\n",
    "                    input_tensor, input_shape,\n",
    "                    pooling, classes,\n",
    "                    **kwargs)\n",
    "\n",
    "\n",
    "def preprocess_input(x, data_format=None, **kwargs):\n",
    "    \"\"\"Preprocesses a numpy array encoding a batch of images.\n",
    "\n",
    "    # Arguments\n",
    "        x: a 3D or 4D numpy array consists of RGB values within [0, 255].\n",
    "        data_format: data format of the image tensor.\n",
    "\n",
    "    # Returns\n",
    "        Preprocessed array.\n",
    "    \"\"\"\n",
    "    return imagenet_utils.preprocess_input(x, data_format,\n",
    "                                           mode='torch', **kwargs)\n",
    "\n",
    "setattr(DenseNet201_hir, '__doc__', DenseNet.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86535247",
   "metadata": {},
   "source": [
    "# models.py\n",
    "This code is based on the git hub repository of: \n",
    "\n",
    "Verma, M., Kumawat, S., Nakashima, Y., & Raman, S. (2020). Yoga-82: a new dataset for fine-grained classification of human poses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (pp. 1038-1039).\n",
    "\n",
    "https://github.com/maniver7/yoga-82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e7f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, blocks, name):\n",
    "    \"\"\"A dense block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, 32, name=name + '_block' + str(i + 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    \"\"\"A building block for a dense block.\n",
    "    # Arguments\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 #if backend.image_data_format() == 'channels_last' else 1\n",
    "    x1 = BatchNormalization(axis=bn_axis,\n",
    "                                   epsilon=1.001e-5,\n",
    "                                   name=name + '_0_bn')(x)\n",
    "    x1 = Activation('relu', name=name + '_0_relu')(x1)\n",
    "    x1 = Conv2D(4 * growth_rate, 1,\n",
    "                       use_bias=False,\n",
    "                       name=name + '_1_conv')(x1)\n",
    "    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
    "                                   name=name + '_1_bn')(x1)\n",
    "    x1 = Activation('relu', name=name + '_1_relu')(x1)\n",
    "    x1 = Conv2D(growth_rate, 3,\n",
    "                       padding='same',\n",
    "                       use_bias=False,\n",
    "                       name=name + '_2_conv')(x1)\n",
    "    x = Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n",
    "    return x\n",
    "\n",
    "def model_one_class(\n",
    "        input_shape = (224,224,3),\n",
    "        class_6=6,\n",
    "        class_20=20,\n",
    "        class_82=82):\n",
    "    # for results of sota papers\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    base_model= DenseNet121(include_top=False, weights=None, input_tensor = inputs, backend = keras.backend , layers = keras.layers , models = keras.models , utils = keras.utils)\n",
    "    \n",
    "\n",
    "    x=  base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(class_82, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs= [x])\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def dense201_hirar(\n",
    "        input_shape = (224,224,3),\n",
    "        class_6=6,\n",
    "        class_20=20,\n",
    "        class_82=82):\n",
    "    \n",
    "    # for variant 1 in the paper\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    base_model= DenseNet201_hir(include_top=False, weights=None, input_tensor = inputs, backend = keras.backend , layers = keras.layers , models = keras.models , utils = keras.utils)\n",
    "   \n",
    "    [x1,x2,x] = base_model.output\n",
    "\n",
    "    x1 = BatchNormalization( epsilon=1.001e-5, name = 'bn_class6_last')(x1)\n",
    "    x1 = Activation('relu', name='relu_class6_last')(x1)                                                                                                                                                                                                                                                                    \n",
    "    x1 = GlobalAveragePooling2D(name='GAvgPool_class6_last')(x1)\n",
    "    x2 = BatchNormalization( epsilon=1.001e-5, name = 'bn_class20_last')(x2)\n",
    "    x2 = Activation('relu', name='relu_class20_last')(x2)\n",
    "    x2 = GlobalAveragePooling2D(name='GAvgPool_class20_last')(x2)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x1 = Dense(class_6, activation= 'softmax')(x1)\n",
    "    x2 = Dense(class_20, activation= 'softmax')(x2)\n",
    "    x = Dense(class_82, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, [x1,x2,x])\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dense201_hirar_6same20(\n",
    "        input_shape = (224,224,3),\n",
    "        class_6=6,\n",
    "        class_20=20,\n",
    "        class_82=82):\n",
    "    \n",
    "    # for variant 2 in the paper\n",
    "    inputs = Input(input_shape)\n",
    "    base_model= DenseNet201_hir(include_top=False, weights=None, input_tensor = inputs, backend = keras.backend , layers = keras.layers , models = keras.models , utils = keras.utils)\n",
    "\n",
    "    [null,x2,x] = base_model.output\n",
    "\n",
    "    x1 = BatchNormalization(epsilon=1.001e-5, name = 'bn_class6_last')(x2)\n",
    "    x1 = Activation('relu', name='relu_class6_last')(x1)\n",
    "    x1 = GlobalAveragePooling2D(name='GAvgPool_class6_last')(x1)\n",
    "    x2 = BatchNormalization(epsilon=1.001e-5, name = 'bn_class20_last')(x2)\n",
    "    x2 = Activation('relu', name='relu_class20_last')(x2)\n",
    "    x2 = GlobalAveragePooling2D(name='GAvgPool_class20_last')(x2)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x1 = Dense(class_6, activation= 'softmax')(x1)\n",
    "\n",
    "    x2 = Dense(class_20, activation= 'softmax')(x2)\n",
    "\n",
    "    x = Dense(class_82, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, [x1,x2,x])\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def dense201_hirar_new(\n",
    "        input_shape = (224,224,3),\n",
    "        class_6=6,\n",
    "        class_20=20,\n",
    "        class_82=82):\n",
    "\n",
    "    # for variant 3 in the paper\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    base_model= DenseNet201_hir(include_top=False, weights=None, input_tensor = inputs, backend = keras.backend , layers = keras.layers , models = keras.models , utils = keras.utils)\n",
    "    \n",
    "    [x1,x2,x] = base_model.output\n",
    "\n",
    "    x1 = dense_block(x1, 32, name='denseblockClass6')\n",
    "\n",
    "\n",
    "    x1 = BatchNormalization( epsilon=1.001e-5, name = 'bn_class6_last')(x1)\n",
    "    x1 = Activation('relu', name='relu_class6_last')(x1)\n",
    "    x1 = GlobalAveragePooling2D(name='GAvgPool_class6_last')(x1)\n",
    "    x2 = BatchNormalization( epsilon=1.001e-5, name = 'bn_class20_last')(x2)\n",
    "    x2 = Activation('relu', name='relu_class20_last')(x2)\n",
    "    x2 = GlobalAveragePooling2D(name='GAvgPool_class20_last')(x2)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x1 = Dense(class_6, activation= 'softmax')(x1)\n",
    "\n",
    "    x2 = Dense(class_20, activation= 'softmax')(x2)\n",
    "\n",
    "    x = Dense(class_82, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, [x1,x2,x])\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0c189",
   "metadata": {},
   "source": [
    "# train_yoga.py\n",
    "\n",
    "This code is the modification of the code from the git hub repository of: \n",
    "\n",
    "Verma, M., Kumawat, S., Nakashima, Y., & Raman, S. (2020). Yoga-82: a new dataset for fine-grained classification of human poses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (pp. 1038-1039).\n",
    "\n",
    "https://github.com/maniver7/yoga-82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c214e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputs):\n",
    "    inputs /=255\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def process_data(df,img_path,train=True):\n",
    "    num = df.shape[0]\n",
    "    \n",
    "    data = np.zeros((num,224,224,3),dtype='float32')\n",
    "    x1_labels = np.zeros(num,dtype='int')\n",
    "    x2_labels = np.zeros(num,dtype='int')\n",
    "    x3_labels = np.zeros(num,dtype='int')\n",
    "\n",
    "    for i in range(num):\n",
    "        path = df['YogaPoses'].iloc[i] + '/' + df['ImageNumbers'].iloc[i]\n",
    "        \n",
    "        x1_label = df['label of class_6'].iloc[i]\n",
    "        x2_label = df['label of class_20'].iloc[i]\n",
    "        x3_label = df['label of class_82'].iloc[i]\n",
    "\n",
    "        imgs = img_path + path\n",
    "        if train:\n",
    "                image = PIL.Image.open(imgs).convert(\"RGB\")\n",
    "                image = image.resize((224,224), PIL.Image.ANTIALIAS)\n",
    "                data[i][:][:][:] = image\n",
    "                x1_labels[i] = x1_label\n",
    "                x2_labels[i] = x2_label\n",
    "                x3_labels[i] = x3_label\n",
    "            \n",
    "        else:\n",
    "            image = PIL.Image.open(imgs).convert(\"RGB\")\n",
    "            image = image.resize((224,224), PIL.Image.ANTIALIAS)\n",
    "            data[i][:][:][:] = image\n",
    "            x1_labels[i] = x1_label\n",
    "            x2_labels[i] = x2_label\n",
    "            x3_labels[i] = x3_label\n",
    "    \n",
    "    return data, x1_labels, x2_labels, x3_labels\n",
    "\n",
    "\n",
    "def generator_train_batch(df,batch_size,num_classes,img_path):\n",
    "    \n",
    "    class_6 = num_classes[0]\n",
    "    class_20 = num_classes[1]\n",
    "    class_82 = num_classes[2]\n",
    "    while True:\n",
    "        \n",
    "            df_shuffled = df.sample(frac=1, random_state=4)\n",
    "            num = df_shuffled.shape[0]\n",
    "            for i in range(0, num, batch_size):\n",
    "                x_train, x1_labels, x2_labels, x3_labels = process_data(df_shuffled[i:(i+batch_size)],img_path,train=True)\n",
    "                x = preprocess(x_train)\n",
    "                y1 = np_utils.to_categorical(np.array(x1_labels), class_6)\n",
    "                y2 = np_utils.to_categorical(np.array(x2_labels), class_20)\n",
    "                y3 = np_utils.to_categorical(np.array(x3_labels), class_82)\n",
    "                y = [y1,y2,y3]\n",
    "                yield x, y\n",
    "\n",
    "def generator_val_batch(df,batch_size,num_classes,img_path):\n",
    "    class_6 = num_classes[0]\n",
    "    class_20 = num_classes[1]\n",
    "    class_82 = num_classes[2]\n",
    "    \n",
    "    while True:\n",
    "            num = df.shape[0]\n",
    "            for i in range(0,num,batch_size):\n",
    "                y_test,y1_labels, y2_labels, y3_labels = process_data(df[i:(i+batch_size)],img_path,train=False)\n",
    "                x = preprocess(y_test)\n",
    "                y1 = np_utils.to_categorical(np.array(y1_labels), class_6)\n",
    "                y2 = np_utils.to_categorical(np.array(y2_labels), class_20)\n",
    "                y3 = np_utils.to_categorical(np.array(y3_labels), class_82)\n",
    "                test_data = x\n",
    "                y = [y1,y2,y3]\n",
    "                yield test_data, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22107176",
   "metadata": {},
   "source": [
    "# Variant 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72a937a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 65s 8s/step - loss: 8.5096 - dense_loss: 1.8404 - dense_1_loss: 2.9419 - dense_2_loss: 3.7273 - dense_accuracy: 0.1800 - dense_1_accuracy: 0.0800 - dense_2_accuracy: 0.1600 - val_loss: 9.0132 - val_dense_loss: 1.8280 - val_dense_1_loss: 2.9772 - val_dense_2_loss: 4.2080 - val_dense_accuracy: 0.0000e+00 - val_dense_1_accuracy: 0.2000 - val_dense_2_accuracy: 0.2000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 35s 7s/step - loss: 5.6156 - dense_loss: 1.7120 - dense_1_loss: 1.9669 - dense_2_loss: 1.9367 - dense_accuracy: 0.2400 - dense_1_accuracy: 0.3200 - dense_2_accuracy: 0.3000 - val_loss: 8.4149 - val_dense_loss: 1.8406 - val_dense_1_loss: 2.8057 - val_dense_2_loss: 3.7687 - val_dense_accuracy: 0.1000 - val_dense_1_accuracy: 0.2000 - val_dense_2_accuracy: 0.2000\n",
      "FIT ENDED\n",
      "Execution time: 1.678315532207489 minutes\n",
      "2/2 [==============================] - 8s 847ms/step - loss: 8.4149 - dense_3_loss: 1.8406 - dense_4_loss: 2.8057 - dense_5_loss: 3.7687 - dense_3_accuracy: 0.1000 - dense_4_accuracy: 0.2000 - dense_5_accuracy: 0.2000\n",
      "2/2 - 6s - 6s/epoch - 3s/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path = '../Results/Results_KAGGLE_TRY/dense201_hirar/'\n",
    "    img_path = '../data/kaggle_yogaposes/DATASET/ALL/' \n",
    "    \n",
    "    test_df = TryTest_df\n",
    "    train_df = TryTrain_df\n",
    "    \n",
    "    num_classes = [6,20,82]\n",
    "    batch_size = 10 #32  #164 training samples, 82 test\n",
    "    epochs = 2\n",
    "\n",
    "    model = dense201_hirar()\n",
    "    #model.load_weights('weights_betweenhirarModify_lw111_6and20ConnectSame_nopre_mix_.0003.hdf5')\n",
    "\n",
    "    lr = 0.003 # orig= 0.003\n",
    "    sgd = SGD(learning_rate=lr, momentum=0.9, nesterov=False)\n",
    "    #adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    \n",
    "    # fix random seed for reproducibility\n",
    "    #seed = 7\n",
    "    #tf.random.set_seed(seed)\n",
    "    \n",
    "    \n",
    "    #model.summary()\n",
    "    # Open the file\n",
    "    #with open(filename + 'modelSummary_V2.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        #model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "    csv_logger= CSVLogger(path + 'log_V1.csv')\n",
    "    #callbacks = [checkpoint]\n",
    "\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "    #save the Keras model or model weights at some frequency.\n",
    "    # \"val_loss\" to monitor the model's total loss.\n",
    "    #verbose: 1 displays messages when the callback takes an action.\n",
    "    model.fit_generator(generator_train_batch(train_df, batch_size, num_classes,img_path),\n",
    "                          steps_per_epoch = train_df.shape[0] // batch_size + 1,\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[csv_logger],\n",
    "                          validation_data=generator_val_batch(test_df, batch_size,num_classes,img_path),\n",
    "                          validation_steps = test_df.shape[0] // batch_size + 1,\n",
    "                          verbose=1)\n",
    "    np.save(path + 'model_weights_V1.npy',model.get_weights())\n",
    "    print('FIT ENDED')\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "    # get execution time in minutes\n",
    "    res = et - st\n",
    "    final_res = res / 60\n",
    "    print('Execution time:', final_res, 'minutes')\n",
    "\n",
    "    model = dense201_hirar()\n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    model.set_weights(np.load(path + 'model_weights_V1.npy', allow_pickle=True))\n",
    "    \n",
    "    batch_size = 5\n",
    "    test_steps_per_epoch = test_df.shape[0] // 5\n",
    "    \n",
    "    score = model.evaluate_generator(generator_val_batch(test_df,batch_size,num_classes,img_path),steps=test_steps_per_epoch, verbose=1)\n",
    "    np.savetxt(path + 'score_V1.txt', score, fmt = '%d')\n",
    "    ###################################################################\n",
    "    #generate predictions\n",
    "    y_predg = model.predict_generator(generator_val_batch(test_df, batch_size, num_classes, img_path), steps=test_steps_per_epoch, verbose=2)\n",
    "\n",
    "    np.savetxt(path + 'probabilities6_V2.txt', y_predg[0], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities20_V2.txt', y_predg[1], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities82_V2.txt', y_predg[2], fmt = '%d')\n",
    "\n",
    "    predictions6 = np.argmax(y_predg[0],axis=1)\n",
    "    np.savetxt(path + 'predictions6.txt', predictions6, fmt='%d')\n",
    "    predictions20 = np.argmax(y_predg[1],axis=1)\n",
    "    np.savetxt(path + 'predictions20.txt', predictions20, fmt='%d')\n",
    "    predictions82 = np.argmax(y_predg[2],axis=1)\n",
    "    np.savetxt(path + 'predictions82.txt', predictions82, fmt='%d')\n",
    "    #get true labels\n",
    "    y_testlabels6 = np_utils.to_categorical(np.array(test_df['label of class_6']), 6)\n",
    "    y_testlabels20 = np_utils.to_categorical(np.array(test_df['label of class_20']), 20)\n",
    "    y_testlabels82 = np_utils.to_categorical(np.array(test_df['label of class_82']), 82)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels6,axis=1), predictions6, 6)\n",
    "    torch.save(cm, path + 'cm6.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels20,axis=1), predictions20, 20)\n",
    "    torch.save(cm, path + 'cm20.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels82,axis=1), predictions82, 82)\n",
    "    torch.save(cm, path + 'cm82.pt')\n",
    "\n",
    "    #classification reports\n",
    "    \n",
    "    report = classification_report(y_testlabels6.argmax(axis=1), predictions6, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report6.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels20.argmax(axis=1), predictions20, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report20.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels82.argmax(axis=1), predictions82, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report82.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc5285",
   "metadata": {},
   "source": [
    "# Variant 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81b3a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5/5 [==============================] - 76s 10s/step - loss: 8.5189 - dense_6_loss: 1.8945 - dense_7_loss: 2.6774 - dense_8_loss: 3.9470 - dense_6_accuracy: 0.2200 - dense_7_accuracy: 0.2000 - dense_8_accuracy: 0.1600 - val_loss: 9.0639 - val_dense_6_loss: 1.7801 - val_dense_7_loss: 2.9548 - val_dense_8_loss: 4.3291 - val_dense_6_accuracy: 0.2000 - val_dense_7_accuracy: 0.3000 - val_dense_8_accuracy: 0.2000\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 44s 9s/step - loss: 5.4631 - dense_6_loss: 1.5833 - dense_7_loss: 1.8513 - dense_8_loss: 2.0286 - dense_6_accuracy: 0.2800 - dense_7_accuracy: 0.4800 - dense_8_accuracy: 0.2800 - val_loss: 8.6725 - val_dense_6_loss: 1.7713 - val_dense_7_loss: 2.8263 - val_dense_8_loss: 4.0749 - val_dense_6_accuracy: 0.3000 - val_dense_7_accuracy: 0.3000 - val_dense_8_accuracy: 0.2000\n",
      "FIT ENDED\n",
      "Execution time: 2.0173866311709086 minutes\n",
      "2/2 [==============================] - 9s 1s/step - loss: 8.6725 - dense_9_loss: 1.7713 - dense_10_loss: 2.8263 - dense_11_loss: 4.0749 - dense_9_accuracy: 0.3000 - dense_10_accuracy: 0.3000 - dense_11_accuracy: 0.2000\n",
      "2/2 - 7s - 7s/epoch - 3s/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path = '../Results/Results_KAGGLE_TRY/dense201_hirar_6same20/'\n",
    "    img_path = '../data/kaggle_yogaposes/DATASET/ALL/' \n",
    "    \n",
    "    test_df = TryTest_df\n",
    "    train_df = TryTrain_df\n",
    "    \n",
    "    num_classes = [6,20,82]\n",
    "    batch_size = 10 #32  #164 training samples, 82 test\n",
    "    epochs = 2\n",
    "\n",
    "    model = dense201_hirar_6same20()\n",
    "\n",
    "    lr = 0.003 \n",
    "    sgd = SGD(learning_rate=lr, momentum=0.9, nesterov=False)\n",
    "    #adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    # Open the file\n",
    "    #with open(filename + 'modelSummary_V2.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        #model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "    csv_logger= CSVLogger(path + 'log_V3.csv')\n",
    "    #callbacks = [checkpoint]\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "    #save the Keras model or model weights at some frequency.\n",
    "    # \"val_loss\" to monitor the model's total loss.\n",
    "    #verbose: 1 displays messages when the callback takes an action.\n",
    "    model.fit_generator(generator_train_batch(train_df, batch_size, num_classes,img_path),\n",
    "                          steps_per_epoch = train_df.shape[0] // batch_size + 1,\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[csv_logger],\n",
    "                          validation_data=generator_val_batch(test_df, batch_size,num_classes,img_path),\n",
    "                          validation_steps = test_df.shape[0] // batch_size + 1,\n",
    "                          verbose=1)\n",
    "    np.save(path + 'model_weights_V1.npy',model.get_weights())\n",
    "    print('FIT ENDED')\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "    # get execution time in minutes\n",
    "    res = et - st\n",
    "    final_res = res / 60\n",
    "    print('Execution time:', final_res, 'minutes')\n",
    "\n",
    "    model = dense201_hirar_6same20()\n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    model.set_weights(np.load(path + 'model_weights_V1.npy', allow_pickle=True))\n",
    "    \n",
    "    batch_size = 5\n",
    "    test_steps_per_epoch = test_df.shape[0] // 5\n",
    "    \n",
    "    score = model.evaluate_generator(generator_val_batch(test_df,batch_size,num_classes,img_path),steps=test_steps_per_epoch, verbose=1)\n",
    "    np.savetxt(path + 'score_V1.txt', score, fmt = '%d')\n",
    "    ###################################################################\n",
    "    #generate predictions\n",
    "    y_predg = model.predict_generator(generator_val_batch(test_df, batch_size, num_classes, img_path), steps=test_steps_per_epoch, verbose=2)\n",
    "\n",
    "    np.savetxt(path + 'probabilities6_V1.txt', y_predg[0], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities20_V1.txt', y_predg[1], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities82_V1.txt', y_predg[2], fmt = '%d')\n",
    "\n",
    "    predictions6 = np.argmax(y_predg[0],axis=1)\n",
    "    np.savetxt(path + 'predictions6.txt', predictions6, fmt='%d')\n",
    "    predictions20 = np.argmax(y_predg[1],axis=1)\n",
    "    np.savetxt(path + 'predictions20.txt', predictions20, fmt='%d')\n",
    "    predictions82 = np.argmax(y_predg[2],axis=1)\n",
    "    np.savetxt(path + 'predictions82.txt', predictions82, fmt='%d')\n",
    "    #get true labels\n",
    "    y_testlabels6 = np_utils.to_categorical(np.array(test_df['label of class_6']), 6)\n",
    "    y_testlabels20 = np_utils.to_categorical(np.array(test_df['label of class_20']), 20)\n",
    "    y_testlabels82 = np_utils.to_categorical(np.array(test_df['label of class_82']), 82)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels6,axis=1), predictions6, 6)\n",
    "    torch.save(cm, path + 'cm6.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels20,axis=1), predictions20, 20)\n",
    "    torch.save(cm, path + 'cm20.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels82,axis=1), predictions82, 82)\n",
    "    torch.save(cm, path + 'cm82.pt')\n",
    "\n",
    "    #classification reports\n",
    "    \n",
    "    report = classification_report(y_testlabels6.argmax(axis=1), predictions6, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report6.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels20.argmax(axis=1), predictions20, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report20.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels82.argmax(axis=1), predictions82, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report82.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaeaab0",
   "metadata": {},
   "source": [
    "# Variant 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59fe53b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = '../Results/Results_KAGGLE_TRY/dense201_hirar_new/'\n",
    "    img_path = '../data/kaggle_yogaposes/DATASET/ALL/' \n",
    "    \n",
    "    test_df = TryTest_df\n",
    "    train_df = TryTrain_df\n",
    "    \n",
    "    num_classes = [6,20,82]\n",
    "    batch_size = 10 #32  #164 training samples, 82 test\n",
    "    epochs = 2\n",
    "\n",
    "    model = dense201_hirar_new()\n",
    "\n",
    "    lr = 0.003 # orig= 0.003\n",
    "    sgd = SGD(learning_rate=lr, momentum=0.9, nesterov=False)\n",
    "    #adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    # Open the file\n",
    "    #with open(filename + 'modelSummary_V2.txt','w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        #model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "    csv_logger= CSVLogger(path + 'log_V3.csv')\n",
    "    #callbacks = [checkpoint]\n",
    "\n",
    "\n",
    "    # get the start time\n",
    "    st = time.time()\n",
    "    #save the Keras model or model weights at some frequency.\n",
    "    # \"val_loss\" to monitor the model's total loss.\n",
    "    #verbose: 1 displays messages when the callback takes an action.\n",
    "    model.fit_generator(generator_train_batch(train_df, batch_size, num_classes,img_path),\n",
    "                          steps_per_epoch = train_df.shape[0] // batch_size + 1,\n",
    "                          epochs=epochs,\n",
    "                          callbacks=[csv_logger],\n",
    "                          validation_data=generator_val_batch(test_df, batch_size,num_classes,img_path),\n",
    "                          validation_steps = test_df.shape[0] // batch_size + 1,\n",
    "                          verbose=1)\n",
    "    np.save(path + 'model_weights_V3.npy',model.get_weights())\n",
    "    print('FIT ENDED')\n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "    # get execution time in minutes\n",
    "    res = et - st\n",
    "    final_res = res / 60\n",
    "    print('Execution time:', final_res, 'minutes')\n",
    "\n",
    "    model = dense201_hirar_new()\n",
    "    model.compile(loss=['categorical_crossentropy','categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1,1], optimizer= sgd, metrics=['accuracy'])#, 'top_k_categorical_accuracy'])\n",
    "    model.set_weights(np.load(path + 'model_weights_V3.npy', allow_pickle=True))\n",
    "    \n",
    "    batch_size = 5\n",
    "    test_steps_per_epoch = test_df.shape[0] // 5\n",
    "    \n",
    "    score = model.evaluate_generator(generator_val_batch(test_df,batch_size,num_classes,img_path),steps=test_steps_per_epoch, verbose=1)\n",
    "    np.savetxt(path + 'score_V2.txt', score, fmt = '%d')\n",
    "    ###################################################################\n",
    "    #generate predictions\n",
    "    y_predg = model.predict_generator(generator_val_batch(test_df, batch_size, num_classes, img_path), steps=test_steps_per_epoch, verbose=2)\n",
    "\n",
    "    np.savetxt(path + 'probabilities6_V3.txt', y_predg[0], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities20_V3.txt', y_predg[1], fmt = '%d')\n",
    "    np.savetxt(path + 'probabilities82_V3.txt', y_predg[2], fmt = '%d')\n",
    "\n",
    "    predictions6 = np.argmax(y_predg[0],axis=1)\n",
    "    np.savetxt(path + 'predictions6.txt', predictions6, fmt='%d')\n",
    "    predictions20 = np.argmax(y_predg[1],axis=1)\n",
    "    np.savetxt(path + 'predictions20.txt', predictions20, fmt='%d')\n",
    "    predictions82 = np.argmax(y_predg[2],axis=1)\n",
    "    np.savetxt(path + 'predictions82.txt', predictions82, fmt='%d')\n",
    "    #get true labels\n",
    "    y_testlabels = np_utils.to_categorical(np.array(test_df['label of class_6']), 6)\n",
    "    \n",
    "\n",
    "    # confusion matrix\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels,axis=1), predictions6, 6)\n",
    "    torch.save(cm, path + 'cm6.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels,axis=1), predictions20, 20)\n",
    "    torch.save(cm, path + 'cm20.pt')\n",
    "    cm = tf.math.confusion_matrix(np.argmax(y_testlabels,axis=1), predictions82, 82)\n",
    "    torch.save(cm, path + 'cm82.pt')\n",
    "\n",
    "    #classification reports\n",
    "    \n",
    "    report = classification_report(y_testlabels.argmax(axis=1), predictions6, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report6.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels.argmax(axis=1), predictions20, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report20.csv')\n",
    "\n",
    "    report = classification_report(y_testlabels.argmax(axis=1), predictions82, output_dict=True)\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(path + 'classification_report82.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
